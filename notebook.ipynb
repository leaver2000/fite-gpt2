{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fite.main import Engine, HyperParameterStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Engine[\"TAF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in HyperParameterStrategy:\n",
    "    results, *_ = pipeline.generate_forecast(\"TAF KDAA 282100Z 2821/3003 32010G15KT 3200\", strategy=strategy)\n",
    "    results = '\\n '.join(results)\n",
    "    print(f\"\"\"{strategy.name=} {strategy.value=}\\n{results}\\n\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7bc700dd09388bc4\n",
      "Found cached dataset json (/home/leaver2000/.cache/huggingface/datasets/json/default-7bc700dd09388bc4/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion', 'metadata'],\n",
      "        num_rows: 19461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion', 'metadata'],\n",
      "        num_rows: 4866\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Any\n",
    "from pathlib import Path\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from datasets.arrow_dataset import Dataset as ArrowDataset\n",
    "from datasets.dataset_dict import DatasetDict  \n",
    "from typing import TypeVar, Generic, Literal, Iterable\n",
    "\n",
    "_KT = TypeVar(\"_KT\")\n",
    "_VT_co = TypeVar(\"_VT_co\", covariant=True)\n",
    "\n",
    "class JSONLine(TypedDict):\n",
    "    \"\"\"A JSON Lines file\"\"\"\n",
    "    metadata: Any\n",
    "    prompt:str\n",
    "    completion: str\n",
    "\n",
    "\n",
    "DatasetDictType = TypedDict(\"DatasetDictType\", {\"train\": list[JSONLine], \"validation\": list[JSONLine], \"test\": list[JSONLine]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dataset(ArrowDataset):\n",
    "    \"\"\"A dataset dictionary\"\"\"\n",
    "\n",
    "    def __getitem__(self, __key: Literal[\"train\", \"test\", \"validate\"]) -> ArrowDataset:\n",
    "        return super().__getitem__(__key) # type: ignore\n",
    "    \n",
    "        \n",
    "    @classmethod\n",
    "    def from_json(cls, path: str, split: float = 0.2, shuffle: bool = True) -> \"DatasetDict\":\n",
    "        return ArrowDataset.from_json(path).train_test_split(test_size=split, shuffle=shuffle) # type: ignore\n",
    "\n",
    "class TAFDataset(DatasetDict):\n",
    "    def forward_metadata(self, s:pd.Series):\n",
    "        return s.str.extract(r\"\\sTX?(?P<max_temp>M?\\d{2})\\/\\d{4}Z\\sTN?(?P<min_temp>M?\\d{2})\\/\\d{4}Z$\")\n",
    "\n",
    "from src.fite.util import SpecialTokens\n",
    "TAFDataset.from_json(\"store/gpt2-taf-base1/training-data.jsonl\")\n",
    "import toml\n",
    "@dataclasses.dataclass\n",
    "class RawTextFileHandler:\n",
    "    \"\"\"A class to handle raw text files\"\"\"\n",
    "    path: Path\n",
    "    split_pattern: str = r\"\\n+###+\\n+\"\n",
    "    split: float = 0.2\n",
    "    shuffle: bool = True\n",
    "    def metadata_handle(self, _:pd.Series) -> pd.DataFrame:\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __post_init__(self):\n",
    "        import re\n",
    "        sep = re.compile(self.split_pattern)\n",
    "        with self.path.open(\"r\") as f:\n",
    "            s = pd.Series(sep.split(f.read()), name=\"text\").str.strip()\n",
    "\n",
    "        df = pd.DataFrame(s.to_frame().join(self.metadata_handle(s)).pipe(self._generate_jsonl),\n",
    "        columns=[\"prompt\", \"completion\", \"metadata\"]\n",
    "        ).drop_duplicates(ignore_index=True)\n",
    "\n",
    "        self._frame = df\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_jsonl(df:pd.DataFrame) -> Iterable[tuple[str,str,str]]:\n",
    "        \"\"\"\n",
    "        iterate over the rows splitting each word, the split text is used to create the prompt and completion.\n",
    "        the __text__ is popped from each dict to create the prompt and completion.\n",
    "        the remaining dict is used as the metadata.\n",
    "        \"\"\"\n",
    "        df[\"text\"] = df.text.str.strip().str.split()\n",
    "        df.columns =  df.columns.str.replace(\"_\", \"-\")\n",
    "        # print(df.set_index([col for col in df.columns if col != \"text\"]).text.items())\n",
    "        \n",
    "        \n",
    "        \n",
    "        for _, metadata in df.iterrows():\n",
    "            prompt = SpecialTokens.bos_token\n",
    "            text_list = metadata.pop(\"text\") \n",
    "            metadata = (f\"{SpecialTokens.metadata}\\n\" + '\\n'.join(f'{k} = {v}' for k, v in metadata.items()))\n",
    "            \n",
    "            for i, text in enumerate(text_list):\n",
    "                prompt += f\"{text} \"\n",
    "                completion = \" \".join(text_list[i  :]) + SpecialTokens.eos_token\n",
    "                yield metadata, prompt, completion\n",
    "\n",
    "\n",
    "    def to_dataset(self) -> DatasetDict:\n",
    "        return ArrowDataset.from_pandas(self._frame).train_test_split(test_size=self.split, shuffle=self.shuffle)\n",
    "\n",
    "class TAFTextFile(RawTextFileHandler):\n",
    "    def metadata_handle(self, s:pd.Series) -> pd.DataFrame:\n",
    "        return s.str.extract(r\"\\sTX?(?P<maximum_temperature>M?\\d{2})\\/\\d{4}Z\\sTN?(?P<minimum_temperature>M?\\d{2})\\/\\d{4}Z$\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = (TAFTextFile(Path(\"store/gpt2-taf-base1/training-data.txt\")).to_dataset())#[\"train\"].to_pandas()#.to_pandas(\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fite-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d13ec20a4f0be8a7bff8acb0062f04d9a58c5cf0e3425f7fa9dc5c43d3ff805"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
